{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import chardet\n",
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    Doc\n",
        ")\n",
        "import os\n",
        "\n",
        "def prepare_ner_data(input_file='ner_results.csv'):\n",
        "    # Check if input file exists\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"Error: Input file '{input_file}' not found in the working directory.\")\n",
        "        return None, None\n",
        "\n",
        "    # Initialize Natasha components for lemmatization\n",
        "    try:\n",
        "        segmenter = Segmenter()\n",
        "        morph_vocab = MorphVocab()\n",
        "        emb = NewsEmbedding()\n",
        "        morph_tagger = NewsMorphTagger(emb)\n",
        "    except Exception as e:\n",
        "        print(f\"Error initializing Natasha components: {e}\")\n",
        "        print(\"Ensure natasha is installed: pip install natasha\")\n",
        "        return None, None\n",
        "\n",
        "    # Function to normalize Russian words (lemmatization)\n",
        "    def normalize_russian_word(word):\n",
        "        try:\n",
        "            # Create a Natasha Doc object for the word\n",
        "            doc = Doc(word)\n",
        "            doc.segment(segmenter)\n",
        "            doc.tag_morph(morph_tagger)\n",
        "\n",
        "            # Get the first token (assuming single word or phrase)\n",
        "            if doc.tokens:\n",
        "                token = doc.tokens[0]\n",
        "                token.lemmatize(morph_vocab)\n",
        "                return token.lemma if token.lemma else word.lower()\n",
        "            return word.lower()\n",
        "        except Exception as e:\n",
        "            print(f\"Error normalizing word '{word}': {e}\")\n",
        "            return word.lower()\n",
        "\n",
        "    # Function to clean and process entities with deduplication\n",
        "    def process_entities(entity_list):\n",
        "        try:\n",
        "            if pd.isna(entity_list) or entity_list == '[]':\n",
        "                return []\n",
        "\n",
        "            # Convert string to list\n",
        "            if isinstance(entity_list, str):\n",
        "                try:\n",
        "                    entities = ast.literal_eval(entity_list)\n",
        "                except:\n",
        "                    # Clean common formatting issues\n",
        "                    cleaned = entity_list.replace('\"[{', '[{').replace('}]\"', '}]')\n",
        "                    try:\n",
        "                        entities = ast.literal_eval(cleaned)\n",
        "                    except:\n",
        "                        return []\n",
        "            else:\n",
        "                entities = entity_list\n",
        "\n",
        "            # Deduplicate with normalized keys\n",
        "            seen = set()\n",
        "            unique_entities = []\n",
        "\n",
        "            for entity in entities:\n",
        "                if not isinstance(entity, dict):\n",
        "                    continue\n",
        "\n",
        "                entity_text = entity.get('text', '')\n",
        "                entity_type = entity.get('type', '')\n",
        "\n",
        "                if not entity_text or not entity_type:\n",
        "                    continue\n",
        "\n",
        "                # Normalize text for deduplication\n",
        "                norm_text = normalize_russian_word(entity_text.lower()) if entity_type in ['PER', 'LOC'] else entity_text.lower()\n",
        "                entity_key = (norm_text, entity_type)\n",
        "\n",
        "                if entity_key not in seen:\n",
        "                    seen.add(entity_key)\n",
        "                    unique_entities.append({\n",
        "                        'text': entity_text,  # Preserve original casing\n",
        "                        'type': entity_type\n",
        "                    })\n",
        "\n",
        "            return unique_entities\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing entities: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    # Read and process the data\n",
        "    try:\n",
        "        # Detect file encoding\n",
        "        with open(input_file, 'rb') as f:\n",
        "            rawdata = f.read(10000)\n",
        "        encoding = chardet.detect(rawdata)['encoding'] or 'utf-8'\n",
        "        print(f\"Detected encoding: {encoding}\")\n",
        "\n",
        "        # Read the CSV with semicolon delimiter\n",
        "        df = pd.read_csv(input_file, encoding='windows-1251', on_bad_lines='skip', sep=';', usecols=['text', 'entities'])\n",
        "\n",
        "        # Print actual column names\n",
        "        print(\"Actual columns in the file:\", df.columns.tolist())\n",
        "\n",
        "        # Clean column names (remove extra semicolons or trailing characters)\n",
        "        df.columns = [col.split(';')[0] for col in df.columns]\n",
        "        print(\"Cleaned columns:\", df.columns.tolist())        # Check if required columns exist\n",
        "        required_columns = ['text', 'entities']\n",
        "        missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "        if missing_columns:\n",
        "            print(f\"Error: Missing required columns: {missing_columns}\")\n",
        "            return None, None\n",
        "\n",
        "        # Print first few rows of raw data\n",
        "        print(\"First few rows of raw data:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Clean entities and create counts\n",
        "        df['entities_clean'] = df['entities'].apply(process_entities)\n",
        "\n",
        "        # Create counts with deduplicated entities\n",
        "        df['per_count'] = df['entities_clean'].apply(lambda x: len([e for e in x if e.get('type') == 'PER']))\n",
        "        df['loc_count'] = df['entities_clean'].apply(lambda x: len([e for e in x if e.get('type') == 'LOC']))\n",
        "        df['org_count'] = df['entities_clean'].apply(lambda x: len([e for e in x if e.get('type') == 'ORG']))\n",
        "\n",
        "        # Create flattened dataframe for dashboard use\n",
        "        entity_rows = []\n",
        "        for _, row in df.iterrows():\n",
        "            for entity in row['entities_clean']:\n",
        "                entity_rows.append({\n",
        "                    'original_text': row['text'],\n",
        "                    'entity_text': entity['text'],\n",
        "                    'entity_type': entity['type']\n",
        "                })\n",
        "\n",
        "        entity_df = pd.DataFrame(entity_rows)\n",
        "\n",
        "        # Save results\n",
        "        df.to_csv('processed_ner_results.csv', index=False, sep='\\t', encoding='utf-8')\n",
        "        entity_df.to_csv('flattened_ner_entities.csv', index=False, sep='\\t', encoding='utf-8')\n",
        "\n",
        "        print(\"Data processing complete with improved deduplication\")\n",
        "        return df, entity_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "# Run the processing\n",
        "sample_df, sample_entities = prepare_ner_data()\n",
        "if sample_df is not None:\n",
        "    print(sample_df[['text', 'entities', 'entities_clean', 'per_count', 'loc_count', 'org_count']].head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPAbcHrt9Gqe",
        "outputId": "e8c686e4-7cbc-43f3-f41e-244290cea954"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected encoding: windows-1251\n",
            "Actual columns in the file: ['text', 'entities']\n",
            "Cleaned columns: ['text', 'entities']\n",
            "First few rows of raw data:\n",
            "                                                text  \\\n",
            "0  Взрыв газа произошел в одной из квартир пятиэт...   \n",
            "1  В Прионежском районе Карелии шесть снегоходов ...   \n",
            "2  Министерство обороны Франции подтвердило, что ...   \n",
            "3  Соединенные Штаты вывели из-под санкций ряд фи...   \n",
            "4  Министр юстиции Сирии Шади Аль-Вайси, назначен...   \n",
            "\n",
            "                                            entities  \n",
            "0  [{'text': 'Луначарского', 'type': 'LOC'}, {'te...  \n",
            "1  [{'text': 'Прионежском районе', 'type': 'LOC'}...  \n",
            "2  [{'text': 'Министерство обороны', 'type': 'ORG...  \n",
            "3  [{'text': 'Соединенные Штаты', 'type': 'LOC'},...  \n",
            "4  [{'text': 'Сирии', 'type': 'LOC'}, {'text': 'Ш...  \n",
            "Data processing complete with improved deduplication\n",
            "                                                text  \\\n",
            "0  Взрыв газа произошел в одной из квартир пятиэт...   \n",
            "\n",
            "                                            entities  \\\n",
            "0  [{'text': 'Луначарского', 'type': 'LOC'}, {'te...   \n",
            "\n",
            "                                      entities_clean  per_count  loc_count  \\\n",
            "0  [{'text': 'Луначарского', 'type': 'LOC'}, {'te...          4          9   \n",
            "\n",
            "   org_count  \n",
            "0          7  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KXcyzsogBBZM"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}